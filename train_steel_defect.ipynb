{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on steel defects dataset\n",
    "This notebook is adapted from Matterport's Mask R-CNN keras implementation ([GitHub](https://github.com/matterport/Mask_RCNN)) for training on shapes. \n",
    "\n",
    ">This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour. The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"/u/home/y/y1lo/project-miao/data_science/detectron/Mask_RCNN/\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SteelDefectsConfig(Config):\n",
    "    \"\"\"Configuration for training on the steel defects dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"steel-defects\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 1600\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load and prepare steel defects dataset.\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDefectsDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def rle2masks(img, masks_enc):\n",
    "    '''decode run-length encoding (RLE) in EncodedPixels and generate 2D defect mask\n",
    "    INPUT: img: 2D nparray, masks_enc: df containing encoded mask pixels\n",
    "    OUTPUT: masks: 3D nparray with 4 defect masks (dim 3)\n",
    "    '''\n",
    "    sr, sc, sz = img.shape\n",
    "    print(sr, sc, sz)\n",
    "    masks = np.zeros((sr, sc, 4), dtype=np.uint8)\n",
    "    # generate 2D mask from RLE mask\n",
    "    for i in range(masks_enc.shape[0]):\n",
    "        mask_enc = masks_enc.iloc[i]\n",
    "        mask_idx = np.array([], dtype=int)\n",
    "        # decode RLE\n",
    "        if isinstance(mask_enc, str):\n",
    "            encoded_array = np.asarray(mask_enc.split()).astype(int)\n",
    "            start_pos = encoded_array[0::2]\n",
    "            run_len = encoded_array[1::2]\n",
    "            for idx in range(len(start_pos)):\n",
    "                curr_idx = np.arange(start_pos[idx], start_pos[idx]+run_len[idx]+1, 1)\n",
    "                mask_idx = np.concatenate((mask_idx, curr_idx))\n",
    "        elif isinstance(mask_enc, float):\n",
    "            mask_id = 0\n",
    "        mask = np.zeros((sr, sc)).flatten()\n",
    "        mask[mask_idx] = 1            \n",
    "        masks[:,:,i] = mask.reshape((sr, sc), order='F')\n",
    "    return masks\n",
    "\n",
    "    def masks2rle(mask):\n",
    "        '''(TODO) encode RLE of mask for submission'''\n",
    "        pass\n",
    "\n",
    "    def display_data(img, masks):\n",
    "        '''show steel image overlaid with contours of detect mask'''\n",
    "        palet = [(255, 255, 0), (0, 255, 255), (255, 0, 255), (0, 255, 0)]\n",
    "        for i in range(masks.shape[2]):\n",
    "            contours, _ = cv2.findContours(masks[:,:,i], cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "            for ii in range(len(contours)):\n",
    "                cv2.polylines(img, contours[ii], True, palet[i], 2)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.draw()\n",
    "\n",
    "    def load_mask(df, fdir, img_id):\n",
    "        '''decode RLE and generate 2D masks if it exists'''\n",
    "        img = cv2.imread(fdir + img_id)\n",
    "        masks_enc = df[df['ImageID'] == img_id]['EncodedPixels']\n",
    "        masks = rle2masks(img, masks_enc)\n",
    "        display_data(img, masks)\n",
    "        return masks.astype(np.bool)\n",
    "\n",
    "        \n",
    "    # shapes dataset\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /u/project/miao/y1lo/data_science/detectron/Mask_RCNN/logs/shapes20190925T1830/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 99/100 [============================>.] - ETA: 5s - loss: 1.8399 - rpn_class_loss: 0.0306 - rpn_bbox_loss: 0.6095 - mrcnn_class_loss: 0.3213 - mrcnn_bbox_loss: 0.4202 - mrcnn_mask_loss: 0.4584 "
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /u/project/miao/y1lo/data_science/detectron/Mask_RCNN/logs/shapes20190925T1750/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/y/y1lo/project-miao/.conda/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "100/100 [==============================] - 1039s 10s/step - loss: 0.8601 - rpn_class_loss: 0.0179 - rpn_bbox_loss: 0.4419 - mrcnn_class_loss: 0.1124 - mrcnn_bbox_loss: 0.1378 - mrcnn_mask_loss: 0.1501 - val_loss: 0.8224 - val_rpn_class_loss: 0.0177 - val_rpn_bbox_loss: 0.4562 - val_mrcnn_class_loss: 0.1031 - val_mrcnn_bbox_loss: 0.1291 - val_mrcnn_mask_loss: 0.1162\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /u/project/miao/y1lo/data_science/detectron/Mask_RCNN/logs/shapes20190925T1750/mask_rcnn_shapes_0002.h5\n",
      "Re-starting from epoch 2\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.975\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-keras] *",
   "language": "python",
   "name": "conda-env-.conda-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
